{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice: Basic Artificial Neural Networks\n",
    "Credits: this notebook belongs to [Practical DL](https://docs.google.com/forms/d/e/1FAIpQLScvrVtuwrHSlxWqHnLt1V-_7h2eON_mlRR6MUb3xEe5x9LuoA/viewform?usp=sf_link) course by Yandex School of Data Analysis.\n",
    "\n",
    "We will start working with neural networks on the practice session. Your homework will be to finish the implementation of the layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is simple, yet an actual implementation may take some time :). We are going to write an Artificial Neural Network (almost) from scratch. The software design was heavily inspired by [PyTorch](http://pytorch.org) which is the main framework of our course "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speaking about the homework (once again, it will be really similar to this seminar), it requires sending **multiple** files, please do not forget to include all the files when sending to TA. The list of files:\n",
    "- This notebook\n",
    "- modules.ipynb with all blocks implemented (except maybe `Conv2d` and `MaxPool2d` layers implementation which are part of 'advanced' version of this homework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:17:27.499703Z",
     "start_time": "2021-12-05T18:17:27.283888Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from time import time, sleep\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:17:28.489963Z",
     "start_time": "2021-12-05T18:17:27.829768Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:17:28.496010Z",
     "start_time": "2021-12-05T18:17:28.494309Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Type, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:17:28.501852Z",
     "start_time": "2021-12-05T18:17:28.500069Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:17:28.628137Z",
     "start_time": "2021-12-05T18:17:28.626113Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement everything in `modules.ipynb`. Read all the comments thoughtfully to ease the pain. Please try not to change the prototypes.\n",
    "\n",
    "Do not forget, that each module should return **AND** store `output` and `gradInput`.\n",
    "\n",
    "The typical assumption is that `module.backward` is always executed after `module.forward`,\n",
    "so `output` is stored, this would be useful for `SoftMax`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tech note\n",
    "Prefer using `np.multiply`, `np.add`, `np.divide`, `np.subtract` instead of `*`,`+`,`/`,`-` for better memory handling.\n",
    "\n",
    "Example: suppose you allocated a variable \n",
    "\n",
    "```\n",
    "a = np.zeros(...)\n",
    "```\n",
    "So, instead of\n",
    "```\n",
    "a = b + c  # will be reallocated, GC needed to free\n",
    "``` \n",
    "You can use: \n",
    "```\n",
    "np.add(b,c,out = a) # puts result in `a`\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:17:32.076537Z",
     "start_time": "2021-12-05T18:17:31.634409Z"
    }
   },
   "outputs": [],
   "source": [
    "# (re-)load layers\n",
    "%run modules.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this example to debug your code, start with logistic regression and then test other layers. You do not need to change anything here. This code is provided for you to test the layers. Also it is easy to use this code in MNIST task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T14:42:25.232373Z",
     "start_time": "2021-12-02T14:42:25.111799Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "N = 500\n",
    "\n",
    "X1 = np.random.randn(N,2) + np.array([2,2])\n",
    "X2 = np.random.randn(N,2) + np.array([-2,-2])\n",
    "\n",
    "Y = np.concatenate([np.ones(N),np.zeros(N)])[:,None]\n",
    "Y = np.hstack([Y, 1-Y])\n",
    "\n",
    "X = np.vstack([X1,X2])\n",
    "plt.scatter(X[:,0],X[:,1], c = Y[:,0], edgecolors= 'none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a **logistic regression** for debugging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T14:42:26.433744Z",
     "start_time": "2021-12-02T14:42:26.427018Z"
    }
   },
   "outputs": [],
   "source": [
    "net = Sequential()\n",
    "net.add(Linear(2, 2))\n",
    "net.add(LogSoftMax())\n",
    "\n",
    "criterion = ClassNLLCriterion()\n",
    "\n",
    "print(net)\n",
    "\n",
    "# Test something like that then \n",
    "\n",
    "# net = Sequential()\n",
    "# net.add(Linear(2, 4))\n",
    "# net.add(ReLU())\n",
    "# net.add(Linear(4, 2))\n",
    "# net.add(LogSoftMax())\n",
    "# print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with batch_size = 1000 to make sure every step lowers the loss, then try stochastic version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T14:42:27.210652Z",
     "start_time": "2021-12-02T14:42:27.207031Z"
    }
   },
   "outputs": [],
   "source": [
    "# Iptimizer params\n",
    "optimizer_config = {'learning_rate' : 1e-1, 'momentum': 0.9}\n",
    "optimizer_state = {}\n",
    "\n",
    "# Looping params\n",
    "n_epoch = 20\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T14:42:27.687420Z",
     "start_time": "2021-12-02T14:42:27.679549Z"
    }
   },
   "outputs": [],
   "source": [
    "# batch generator\n",
    "def get_batches(dataset, batch_size):\n",
    "    X, Y = dataset\n",
    "    n_samples = X.shape[0]\n",
    "        \n",
    "    # Shuffle at the start of epoch\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_idx = indices[start:end]\n",
    "    \n",
    "        yield X[batch_idx], Y[batch_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic training loop. Examine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T14:42:32.153131Z",
     "start_time": "2021-12-02T14:42:30.478773Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    for x_batch, y_batch in get_batches((X, Y), batch_size):\n",
    "        \n",
    "        net.zeroGradParameters()\n",
    "        \n",
    "        # Forward\n",
    "        predictions = net.forward(x_batch)\n",
    "        loss = criterion.forward(predictions, y_batch)\n",
    "    \n",
    "        # Backward\n",
    "        dp = criterion.backward(predictions, y_batch)\n",
    "        net.backward(x_batch, dp)\n",
    "        \n",
    "        # Update weights\n",
    "        sgd_momentum(net.getParameters(), \n",
    "                     net.getGradParameters(), \n",
    "                     optimizer_config,\n",
    "                     optimizer_state)      \n",
    "        \n",
    "        loss_history.append(loss)\n",
    "\n",
    "    # Visualize\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "        \n",
    "    plt.title(\"Training loss\")\n",
    "    plt.xlabel(\"#iteration\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.plot(loss_history, 'b')\n",
    "    plt.show()\n",
    "    \n",
    "    print('Current loss: %f' % loss)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using old good [MNIST](http://yann.lecun.com/exdb/mnist/) as our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:17:35.857232Z",
     "start_time": "2021-12-05T18:17:35.503264Z"
    }
   },
   "outputs": [],
   "source": [
    "import mnist\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = mnist.load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encode the labels first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T17:45:12.291109Z",
     "start_time": "2021-12-05T17:45:12.288428Z"
    }
   },
   "outputs": [],
   "source": [
    "def OneHotEncoding(y):\n",
    "    \"\"\"\n",
    "    Encoding targets using one hot encoding\n",
    "    \"\"\"\n",
    "    assert len(y.shape) == 1\n",
    "\n",
    "    y_coded = np.zeros((y.shape[0], np.max(np.unique(y)) + 1))\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        y_coded[i][y[i]] = 1\n",
    "    return y_coded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T17:45:13.664423Z",
     "start_time": "2021-12-05T17:45:13.660489Z"
    }
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T17:45:14.174249Z",
     "start_time": "2021-12-05T17:45:14.156178Z"
    }
   },
   "outputs": [],
   "source": [
    "print(OneHotEncoding(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:17:45.588960Z",
     "start_time": "2021-12-05T18:17:45.580651Z"
    }
   },
   "outputs": [],
   "source": [
    "# class DatasetMNIST(Dataset):\n",
    "#     def __init__(self, X, y, transforms=None):\n",
    "#         self.data, self.labels = X, y\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         data = self.data[index]\n",
    "#         label = self.labels[index]\n",
    "\n",
    "#         if self.transform is not None:\n",
    "#             data = self.transforms(data)\n",
    "\n",
    "#         return data, label\n",
    "class DatasetMNIST(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = np.array(X)\n",
    "        self.y = np.array(y)\n",
    "        self.transform = transform\n",
    "        assert (X.shape[0] == y.shape[0])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        obj = self.X[index]\n",
    "        label = self.y[index]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            obj = self.transform(obj)\n",
    "            \n",
    "        return obj, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T17:21:45.389285Z",
     "start_time": "2021-12-05T17:21:45.126831Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = DatasetMNIST(X_train, OneHotEncoding(y_train))\n",
    "val_dataset = DatasetMNIST(X_val, OneHotEncoding(y_val))\n",
    "test_dataset = DatasetMNIST(X_test, OneHotEncoding(y_test))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T17:45:22.273435Z",
     "start_time": "2021-12-05T17:45:22.267053Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Размерность y_true (object_counts, 10)\n",
    "    Размерность y_pred (object_counts, 10)\n",
    "    \"\"\"\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    acc = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        if np.argmax(y_true[i]) == np.argmax(y_pred[i]):\n",
    "            acc += 1\n",
    "    return acc / y_pred.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Compare** `ReLU`, `ELU`, `LeakyReLU`, `SoftPlus` activation functions. \n",
    "You would better pick the best optimizer params for each of them, but it is overkill for now. Use an architecture of your choice for the comparison.\n",
    "- **Try** inserting `BatchNormalization` (folowed by `ChannelwiseScaling`) between `Linear` module and activation functions.\n",
    "- Plot the losses both from activation functions comparison and `BatchNormalization` comparison on one plot. Please find a scale (log?) when the lines are distinguishable, do not forget about naming the axes, the plot should be goodlooking.\n",
    "- Plot the losses for two networks: one trained by momentum_sgd, another one trained by Adam. Which one performs better?\n",
    "- Hint: good logloss for MNIST should be around 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T13:33:33.620632Z",
     "start_time": "2021-12-05T13:33:33.608587Z"
    }
   },
   "outputs": [],
   "source": [
    "def Neural_Network(activation=ReLU,\n",
    "                  input_resolution: Tuple[int, int]=(28, 28),\n",
    "                  input_channels: int=1,\n",
    "                  num_classes: int=10,\n",
    "                  batch_norm: bool=False\n",
    "                  ):\n",
    "    if batch_norm:\n",
    "        in_features = np.product(input_resolution) * input_channels\n",
    "        sequential = Sequential()\n",
    "        flatten = Flatten()\n",
    "        sequential.add(flatten)\n",
    "        linear = Linear(in_features, 256)\n",
    "        sequential.add(linear)\n",
    "        bn = BatchNormalization(0.001)\n",
    "        sequential.add(bn)\n",
    "        cws = ChannelwiseScaling(256)\n",
    "        sequential.add(cws)\n",
    "        activation = activation()\n",
    "        sequential.add(activation)\n",
    "        linear_2 = Linear(256, num_classes)\n",
    "        sequential.add(linear_2)\n",
    "        log_softmax = LogSoftMax()\n",
    "        sequential.add(log_softmax)\n",
    "        return sequential\n",
    "    else:\n",
    "        in_features = np.product(input_resolution) * input_channels\n",
    "        sequential = Sequential()\n",
    "        flatten = Flatten()\n",
    "        sequential.add(flatten)\n",
    "        linear = Linear(in_features, 256)\n",
    "        sequential.add(linear)\n",
    "        activation = activation()\n",
    "        sequential.add(activation)\n",
    "        linear_2 = Linear(256, num_classes)\n",
    "        sequential.add(linear_2)\n",
    "        log_softmax = LogSoftMax()\n",
    "        sequential.add(log_softmax)\n",
    "        return sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T13:33:34.322289Z",
     "start_time": "2021-12-05T13:33:34.311922Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_loop(train_loader,\n",
    "               val_loader,\n",
    "               net,\n",
    "               criterion,\n",
    "               optimizer, \n",
    "               optimizer_config,\n",
    "               batch_size=128,\n",
    "               n_epoch=20):\n",
    "\n",
    "    loss_history = []\n",
    "    loss_val_history = []\n",
    "    optimizer_state = {}\n",
    "\n",
    "    for i in tqdm(range(n_epoch)):\n",
    "        loss = []\n",
    "        loss_val = None\n",
    "        \n",
    "        net.train()\n",
    "        for x_batch, y_batch in train_loader:\n",
    "\n",
    "            net.zeroGradParameters()\n",
    "\n",
    "            # Forward\n",
    "            predictions = net.forward(x_batch.numpy())\n",
    "            loss.append(criterion.forward(predictions, y_batch.numpy()))\n",
    "\n",
    "            # Backward\n",
    "            dp = criterion.backward(predictions, y_batch.numpy())\n",
    "            net.backward(x_batch.numpy(), dp)\n",
    "\n",
    "            # Update weights\n",
    "            sgd_momentum(net.getParameters(), net.getGradParameters(),\n",
    "                         optimizer_config, optimizer_state)\n",
    "\n",
    "            \n",
    "        loss_history.append(np.mean(loss))\n",
    "\n",
    "        print('Epoch: %i' % i)\n",
    "        print('Current mean train loss: %f' % np.mean(loss))\n",
    "\n",
    "    return loss_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T13:36:02.415333Z",
     "start_time": "2021-12-05T13:36:02.411903Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = ClassNLLCriterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T13:33:35.729878Z",
     "start_time": "2021-12-05T13:33:35.675227Z"
    }
   },
   "outputs": [],
   "source": [
    "net_ReLU = Neural_Network(activation=ReLU)\n",
    "net_ReLU_bn = Neural_Network(activation=ReLU, batch_norm=True)\n",
    "\n",
    "net_ELU = Neural_Network(activation=ELU)\n",
    "net_ELU_bn = Neural_Network(activation=ELU, batch_norm=True)\n",
    "\n",
    "net_LeakyReLU = Neural_Network(activation=ELU)\n",
    "net_LeakyReLU_bn = Neural_Network(activation=ELU, batch_norm=True)\n",
    "\n",
    "net_SoftPlus = Neural_Network(activation=SoftPlus)\n",
    "net_SoftPlus_bn = Neural_Network(activation=SoftPlus, batch_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T13:33:43.813966Z",
     "start_time": "2021-12-05T13:33:36.323621Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_ReLU = train_loop(train_loader=train_loader,\n",
    "                       val_loader=val_loader,\n",
    "                       net=net_ReLU,\n",
    "                       criterion=criterion,\n",
    "                       optimizer=sgd_momentum, \n",
    "                       optimizer_config={'learning_rate' : 1e-1, 'momentum': 0.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T13:33:43.831584Z",
     "start_time": "2021-12-05T13:33:37.190Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_ReLU_bn = train_loop(train_loader=train_loader,\n",
    "                          val_loader=val_loader,\n",
    "                          net=net_ReLU_bn,\n",
    "                          criterion=criterion,\n",
    "                          optimizer=sgd_momentum, \n",
    "                          optimizer_config={'learning_rate' : 1e-1, 'momentum': 0.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T13:33:43.843598Z",
     "start_time": "2021-12-05T13:33:37.804Z"
    }
   },
   "outputs": [],
   "source": [
    "display.clear_output(wait=True)\n",
    "plt.figure(figsize=(12, 9))\n",
    "\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(loss_ReLU, 'b', label='ReLU')\n",
    "plt.plot(loss_ReLU_bn, 'g', label='ReLU with BatchNorm')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f'min ReLU loss: {np.min(loss_ReLU)}')\n",
    "print(f'min ReLU with batchnorm loss: {np.min(loss_ReLU_bn)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T13:33:43.855830Z",
     "start_time": "2021-12-05T13:33:38.468Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_ELU = train_loop(train_loader=train_loader,\n",
    "                       val_loader=val_loader,\n",
    "                       net=net_ELU,\n",
    "                       criterion=criterion,\n",
    "                       optimizer=sgd_momentum, \n",
    "                       optimizer_config={'learning_rate' : 1e-1, 'momentum': 0.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T13:33:43.868330Z",
     "start_time": "2021-12-05T13:33:38.903Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_ELU_bn = train_loop(train_loader=train_loader,\n",
    "                         val_loader=val_loader,\n",
    "                         net=net_ELU_bn,\n",
    "                         criterion=criterion,\n",
    "                         optimizer=sgd_momentum, \n",
    "                         optimizer_config={'learning_rate' : 1e-1, 'momentum': 0.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T13:33:43.879350Z",
     "start_time": "2021-12-05T13:33:39.252Z"
    }
   },
   "outputs": [],
   "source": [
    "display.clear_output(wait=True)\n",
    "plt.figure(figsize=(12, 9))\n",
    "\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(loss_ELU, 'b', label='ELU')\n",
    "plt.plot(loss_ELU_bn, 'g', label='ELU with BatchNorm')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f'min ELU loss: {np.min(loss_ELU)}')\n",
    "print(f'min ELU with batchnorm loss: {np.min(loss_ELU_bn)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-04T22:51:16.705918Z",
     "start_time": "2021-12-04T22:40:57.356367Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_LeakyReLU = train_loop(train_loader=train_loader,\n",
    "                             val_loader=val_loader,\n",
    "                             net=net_LeakyReLU,\n",
    "                             criterion=criterion,\n",
    "                             optimizer=sgd_momentum, \n",
    "                             optimizer_config={'learning_rate' : 1e-1, 'momentum': 0.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-04T23:01:42.571619Z",
     "start_time": "2021-12-04T22:51:16.853068Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_LeakyReLU_bn = train_loop(train_loader=train_loader,\n",
    "                                val_loader=val_loader,\n",
    "                                net=net_LeakyReLU_bn,\n",
    "                                criterion=criterion,\n",
    "                                optimizer=sgd_momentum, \n",
    "                                optimizer_config={'learning_rate' : 1e-1, 'momentum': 0.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-04T23:02:36.541957Z",
     "start_time": "2021-12-04T23:02:36.395051Z"
    }
   },
   "outputs": [],
   "source": [
    "display.clear_output(wait=True)\n",
    "plt.figure(figsize=(12, 9))\n",
    "\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(loss_LeakyReLU, 'b', label='LeakyReLU')\n",
    "plt.plot(loss_LeakyReLU_bn, 'g', label='LeakyReLU with BatchNorm')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f'min LeakyReLU loss: {np.min(loss_LeakyReLU)}')\n",
    "print(f'min LeakyReLU with batchnorm loss: {np.min(loss_LeakyReLU_bn)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-04T22:16:48.970502Z",
     "start_time": "2021-12-04T22:06:16.183873Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_SoftPlus = train_loop(train_loader=train_loader,\n",
    "                           val_loader=val_loader,\n",
    "                           net=net_SoftPlus,\n",
    "                           criterion=criterion,\n",
    "                           optimizer=sgd_momentum, \n",
    "                           optimizer_config={'learning_rate' : 1e-1, 'momentum': 0.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-04T22:27:30.012583Z",
     "start_time": "2021-12-04T22:16:51.494137Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_SoftPlus_bn = train_loop(train_loader=train_loader,\n",
    "                              val_loader=val_loader,\n",
    "                              net=net_SoftPlus_bn,\n",
    "                              criterion=criterion,\n",
    "                              optimizer=sgd_momentum, \n",
    "                              optimizer_config={'learning_rate' : 1e-1, 'momentum': 0.9})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-04T22:32:29.042860Z",
     "start_time": "2021-12-04T22:32:28.890847Z"
    }
   },
   "outputs": [],
   "source": [
    "display.clear_output(wait=True)\n",
    "plt.figure(figsize=(12, 9))\n",
    "\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(loss_SoftPlus, 'b', label='SoftPlus')\n",
    "plt.plot(loss_SoftPlus_bn, 'g', label='SoftPlus with BatchNorm')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f'min SoftPlus loss: {np.min(loss_SoftPlus)}')\n",
    "print(f'min SoftPlus with batchnorm loss: {np.min(loss_SoftPlus_bn)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T12:17:17.238768Z",
     "start_time": "2021-12-05T12:17:17.216113Z"
    }
   },
   "outputs": [],
   "source": [
    "net_LeakyReLU_Adagrad_bn = Neural_Network(activation=LeakyReLU, batch_norm=True)\n",
    "net_LeakyReLU_sgd_bn = Neural_Network(activation=LeakyReLU, batch_norm=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T12:28:21.003412Z",
     "start_time": "2021-12-05T12:17:17.726744Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_LeakyReLU_sgd_bn = train_loop(train_loader=train_loader,\n",
    "                           val_loader=val_loader,\n",
    "                           net=net_LeakyReLU_sgd_bn,\n",
    "                           criterion=criterion,\n",
    "                           optimizer=sgd_momentum,\n",
    "                           optimizer_config={'learning_rate' : 1e-1, 'momentum': 0.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T12:39:11.896508Z",
     "start_time": "2021-12-05T12:28:21.157097Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_LeakyReLU_Adagrad_bn = train_loop(train_loader=train_loader,\n",
    "                               val_loader=val_loader,\n",
    "                               net=net_LeakyReLU_Adagrad_bn,\n",
    "                               criterion=criterion,\n",
    "                               optimizer=adam_optimizer,\n",
    "                               optimizer_config={'learning_rate' : 1e-1, 'momentum': 0.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T12:42:39.644041Z",
     "start_time": "2021-12-05T12:42:39.500447Z"
    }
   },
   "outputs": [],
   "source": [
    "display.clear_output(wait=True)\n",
    "plt.figure(figsize=(12, 9))\n",
    "\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(loss_LeakyReLU_sgd_bn, 'b', label='SoftPlus')\n",
    "plt.plot(loss_LeakyReLU_Adagrad_bn, 'g', label='SoftPlus with BatchNorm')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f'min LeakyReLU (BatchNorm) with SGD optimizer loss: {np.min(loss_LeakyReLU_sgd_bn)}')\n",
    "print(f'min LeakyReLU (BatchNorm) with Adam optimizer loss: {np.min(loss_LeakyReLU_Adagrad_bn)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T17:22:19.175621Z",
     "start_time": "2021-12-05T17:22:19.161818Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_loop(train_loader,\n",
    "               val_loader,\n",
    "               net,\n",
    "               criterion,\n",
    "               optimizer, \n",
    "               optimizer_config,\n",
    "               batch_size=128,\n",
    "               n_epoch=20):\n",
    "\n",
    "    loss_history = []\n",
    "    loss_val_history = []\n",
    "    optimizer_state = {}\n",
    "\n",
    "    for i in tqdm(range(n_epoch)):\n",
    "        loss = []\n",
    "        loss_val = []\n",
    "        \n",
    "        net.train()\n",
    "        for x_batch, y_batch in train_loader:\n",
    "\n",
    "            net.zeroGradParameters()\n",
    "\n",
    "            # Forward\n",
    "            predictions = net.forward(x_batch.numpy())\n",
    "            loss.append(criterion.forward(predictions, y_batch.numpy()))\n",
    "\n",
    "            # Backward\n",
    "            dp = criterion.backward(predictions, y_batch.numpy())\n",
    "            net.backward(x_batch.numpy(), dp)\n",
    "\n",
    "            # Update weights\n",
    "            sgd_momentum(net.getParameters(), net.getGradParameters(),\n",
    "                         optimizer_config, optimizer_state)\n",
    "\n",
    "            \n",
    "        loss_history.append(np.mean(loss))\n",
    "\n",
    "        net.evaluate()\n",
    "        for x_val, y_val in val_loader:\n",
    "            y_val_pred = net.forward(x_val.numpy())\n",
    "            loss_val.append(criterion.forward(y_val_pred, y_val.numpy()))\n",
    "\n",
    "        loss_val_history.append(np.mean(loss_val))\n",
    "\n",
    "        print('Epoch: %i' % i)\n",
    "        print('Current mean train loss: %f' % np.mean(loss))\n",
    "        print('Current mean validation loss: %f' % np.mean(loss_val))\n",
    "\n",
    "    return loss_history, loss_val_history\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:19:28.864240Z",
     "start_time": "2021-12-05T18:19:28.857844Z"
    }
   },
   "outputs": [],
   "source": [
    "train_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToPILImage(),\n",
    "    torchvision.transforms.RandomResizedCrop(size=(28, 28), scale=(.5, 1.0), ratio=(.8, 1.25)),\n",
    "    torchvision.transforms.RandomRotation(degrees=30),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T17:45:39.172603Z",
     "start_time": "2021-12-05T17:45:38.975754Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = DatasetMNIST(X_train, OneHotEncoding(y_train), transform=train_transforms)\n",
    "val_dataset = DatasetMNIST(X_val, OneHotEncoding(y_val), transform=test_transforms)\n",
    "test_dataset = DatasetMNIST(X_test, OneHotEncoding(y_test), transform=test_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T17:22:09.658825Z",
     "start_time": "2021-12-05T17:22:09.648155Z"
    }
   },
   "outputs": [],
   "source": [
    "def advanced_NN(activation=LeakyReLU,\n",
    "                  input_resolution: Tuple[int, int]=(28, 28),\n",
    "                  input_channels: int=1,\n",
    "                  num_classes: int=10,\n",
    "                  ):\n",
    "\n",
    "    in_features = np.product(input_resolution) * input_channels\n",
    "    sequential = Sequential()\n",
    "    flatten = Flatten()\n",
    "    sequential.add(flatten)\n",
    "    linear = Linear(in_features, 256)\n",
    "    sequential.add(linear)\n",
    "    bn = BatchNormalization(0.001)\n",
    "    sequential.add(bn)\n",
    "    cws = ChannelwiseScaling(256)\n",
    "    sequential.add(cws)\n",
    "    activation = activation()\n",
    "    sequential.add(activation)\n",
    "    dropout = Dropout()\n",
    "    sequential.add(dropout)\n",
    "    linear_2 = Linear(256, num_classes)\n",
    "    sequential.add(linear_2)\n",
    "    log_softmax = LogSoftMax()\n",
    "    sequential.add(log_softmax)\n",
    "    return sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T17:22:10.183005Z",
     "start_time": "2021-12-05T17:22:10.174904Z"
    }
   },
   "outputs": [],
   "source": [
    "model = advanced_NN()\n",
    "criterion = ClassNLLCriterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T17:22:10.615633Z",
     "start_time": "2021-12-05T17:22:10.613855Z"
    }
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T17:32:44.278434Z",
     "start_time": "2021-12-05T17:22:23.107264Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "loss_train, loss_val = train_loop(train_loader=train_loader,\n",
    "                               val_loader=val_loader,\n",
    "                               net=model,\n",
    "                               criterion=criterion,\n",
    "                               optimizer=adam_optimizer,\n",
    "                               optimizer_config={'learning_rate' : 1e-1, 'momentum': 0.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T17:32:44.915405Z",
     "start_time": "2021-12-05T17:32:44.742754Z"
    }
   },
   "outputs": [],
   "source": [
    "display.clear_output(wait=True)\n",
    "plt.figure(figsize=(12, 9))\n",
    "\n",
    "plt.title(\"Train and validation loss comparison\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(loss_train, 'b', label='train')\n",
    "plt.plot(loss_val, 'g', label='validation')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f'min train loss: {np.min(loss_train)}')\n",
    "print(f'min validation loss: {np.min(loss_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T17:32:49.301735Z",
     "start_time": "2021-12-05T17:32:45.358465Z"
    }
   },
   "outputs": [],
   "source": [
    "model.evaluate()\n",
    "acc = []\n",
    "for x_batch, y_batch in test_loader:\n",
    "    y_pred = model.forward(X_test)\n",
    "    acc.append(accuracy_score(OneHotEncoding(y_test), y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T17:32:49.892085Z",
     "start_time": "2021-12-05T17:32:49.890037Z"
    }
   },
   "outputs": [],
   "source": [
    "print(np.mean(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your personal opinion on the activation functions, think about computation times too. Does `BatchNormalization` help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally**, use all your knowledge to build a super cool model on this dataset. Use **dropout** to prevent overfitting, play with **learning rate decay**. You can use **data augmentation** such as rotations, translations to boost your score. Use your knowledge and imagination to train a model. Don't forget to call `training()` and `evaluate()` methods to set desired behaviour of `BatchNormalization` and `Dropout` layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print here your accuracy on test set. It should be around 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer goes here. ################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing with PyTorch implementation\n",
    "The last (and maybe the easiest step after compared to the previous tasks: build a network with the same architecture as above now with PyTorch.\n",
    "\n",
    "You can refer to the `week0_09` or `Lab3_part2` notebooks for hints.\n",
    "\n",
    "__Good Luck!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:19:12.585475Z",
     "start_time": "2021-12-05T18:19:12.576470Z"
    }
   },
   "outputs": [],
   "source": [
    "def PyTorch_NN(activation=nn.LeakyReLU,\n",
    "               input_resolution: Tuple[int, int]=(28, 28),\n",
    "               input_channels: int=1,\n",
    "               num_classes: int=10):\n",
    "    \n",
    "    in_features = np.product(input_resolution) * input_channels\n",
    "    return nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features, 256),\n",
    "        nn.BatchNorm1d(256, momentum=0.001),\n",
    "        activation(),\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(256, num_classes)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:19:12.995594Z",
     "start_time": "2021-12-05T18:19:12.969026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (2): BatchNorm1d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
      "  (3): LeakyReLU(negative_slope=0.01)\n",
      "  (4): Dropout(p=0.5, inplace=False)\n",
      "  (5): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = PyTorch_NN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:19:13.932124Z",
     "start_time": "2021-12-05T18:19:13.926376Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:19:16.392373Z",
     "start_time": "2021-12-05T18:19:16.388559Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_transforms = torchvision.transforms.Compose([\n",
    "#     torchvision.transforms.ToPILImage(),\n",
    "#     torchvision.transforms.RandomResizedCrop(size=(28, 28), scale=(.5, 1.0), ratio=(.8, 1.25)),\n",
    "#     torchvision.transforms.RandomRotation(degrees=30),\n",
    "#     torchvision.transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# test_transforms = torchvision.transforms.Compose([\n",
    "#     torchvision.transforms.ToTensor(),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:19:35.324754Z",
     "start_time": "2021-12-05T18:19:35.145090Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = DatasetMNIST(X_train, y_train, transform=train_transforms)\n",
    "val_dataset = DatasetMNIST(X_val, y_val, transform=test_transforms)\n",
    "test_dataset = DatasetMNIST(X_test, y_test, transform=test_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T18:19:38.376534Z",
     "start_time": "2021-12-05T18:19:38.364900Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_loop(train_loader,\n",
    "               val_loader,\n",
    "               model,\n",
    "               criterion,\n",
    "               optimizer, \n",
    "               batch_size=128,\n",
    "               n_epoch=20):\n",
    "\n",
    "    loss_history = []\n",
    "    loss_val_history = []\n",
    "\n",
    "    for i in tqdm(range(n_epoch)):\n",
    "        loss = []\n",
    "        loss_val = []\n",
    "        \n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_batch)\n",
    "            loss_cur = criterion(y_pred, y_batch)\n",
    "            loss.append(loss_cur.item())\n",
    "            loss_cur.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            \n",
    "        loss_history.append(np.mean(loss))\n",
    "\n",
    "        model.eval()\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(x_batch)\n",
    "                loss_cur = criterion(y_pred, y_batch)\n",
    "                loss_val.append(loss_cur.item())\n",
    "                \n",
    "        loss_val_history.append(np.mean(loss_val))\n",
    "\n",
    "        print('Epoch: %i' % i)\n",
    "        print('Current mean train loss: %f' % np.mean(loss))\n",
    "        print('Current mean validation loss: %f' % np.mean(loss_val))\n",
    "\n",
    "    return loss_history, loss_val_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-05T18:19:39.444Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69647ab46a3047bc9e6d26cb0e46ade4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_train, loss_val = train_loop(train_loader=train_loader,\n",
    "                                  val_loader=val_loader,\n",
    "                                  model=model,\n",
    "                                  criterion=criterion,\n",
    "                                  optimizer=torch.optim.Adam(model.parameters(), lr=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "batches = 0\n",
    "for x_batch, y_batch in test_loader:\n",
    "    y_pred = model.forward(x_batch)\n",
    "    batches += 1\n",
    "    acc += ((torch.argmax(y_pred, dim=1) == y_batch).numpy().mean())\n",
    "print(acc/batches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
